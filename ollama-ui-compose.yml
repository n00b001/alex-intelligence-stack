services:
    # vllm:
    #     ports:
    #         - 8883:5000
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - /home/alex/.cache/ollama:/root/.ollama
    #         - /home/alex/.cache/huggingface:/root/.cache/huggingface
    #         - /home/alex/.cache/llms:/root/.cache/llms
    #         - /home/alex/.cache/huggingface:/workspace/.cache/huggingface
    #         - vllm-cache:/workspace/.cache
    #     image: vllm/vllm-openai:latest
    #     container_name: vllm
    #     restart: unless-stopped
    #     networks:
    #         - ollama-docker
    #     entrypoint: python3
    #     # https://docs.vllm.ai/en/latest/serving/engine_args.html#engine-args
    #     # command: /bin/bash
    #     command: [
    #         "-m", "vllm.entrypoints.openai.api_server",
    #         "--port=5000","--host=0.0.0.0",
    #         "--trust-remote-code",
    #         "--enable-sleep-mode",
    #         "--seed","0",
    #         # "--api-key","token-abc123",
    #         # "--model","Qwen/Qwen2.5-Coder-14B-Instruct-AWQ",
    #         # "--model","/root/.cache/llms/t2t/QwQ-32B.Q8_0.gguf.gguf",
    #         # "--model","/root/.ollama/models/blobs/sha256-1e8b6e0cc0ca29859e57ab9562237f00a234a464fa472d9c3202db7d97b27855",
    #         # "--tokenizer","Qwen/QwQ-32B",
    #         # "--hf-config-path","Qwen/QwQ-32B",
    #         # "--model","Qwen/Qwen3-0.6B-FP8",

    #         # not oom
    #         # "--model","Qwen/Qwen3-0.6B",

    #         # oom
    #         # "--model","Qwen/Qwen3-30B-A3B",
    #         # "--model","Qwen/Qwen3-30B-A3B-FP8",
    #         # "--model","Qwen/Qwen3-32B",
    #         # "--model","Qwen/Qwen3-32B-FP8",
    #         # "--tokenizer","Qwen/QwQ-32B",
    #         # "--hf-config-path","Qwen/QwQ-32B",
    #         # "--model","Orion-zhen/Qwen3-32B-AWQ",
    #         # "-q","awq",
    #         # "--model","Qwen/Qwen3-14B",
    #         "--model","Qwen/Qwen3-14B-FP8",
    #         # "--model","Qwen3/Qwen3-8B-AWQ",
    #         "--max-model-len","32768",
    #         "--enable-reasoning", "--reasoning-parser","deepseek_r1",
    #         # "--rope-scaling",'{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'
    #         # "--model","/root/.ollama/models/blobs/sha256-1e8b6e0cc0ca29859e57ab9562237f00a234a464fa472d9c3202db7d97b27855",
    #         # "--tokenizer","Qwen/Qwen3-30B-A3B",
    #         # "--hf-config-path","Qwen/Qwen3-30B-A3B",
    #         # "--model","unsloth/QwQ-32B-GGUF",
    #         # "--model","ModelCloud/QwQ-32B-gptqmodel-4bit-vortex-v1",
    #         # "--model","Qwen/QwQ-32B-AWQ",
    #         # "--model","Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4",
    #         # "--model","Qwen/Qwen2.5-Coder-32B-Instruct-AWQ",
    #         # "--model","Vezora/Qwen2.5-Coder-32B-Instruct-fp8-W8A16",
    #         # "--dtype","auto",
    #         # "--dtype","half",
    #         # "--quantization","awq",
    #         # "--quantization","awq_marlin",
    #         # "--quantization","compressed-tensors",
    #         # "--calculate-kv-scales",

    #         # performance improvments
    #         # claude sonet 3.7
    #         # "--max-num-batched-tokens","8192",
    #         # "--block-size","16",
    #         # "--swap-space","16",
    #         # "--numa-aware",
    #         # "--disable-log-stats",

    #         # openai o4-mini
    #         # "--num-cpu-workers","6",
    #         # "--num-gpu-per-worker","1",
    #         # "--max-batch-size","16",
    #         # "--batch-timeout","0",
    #         # "--enable-chunked-prefill",
    #         # "--chunked-prefill-timing-warmup","10ms",
    #         "--tensor-parallel-size","1",
    #         "--pipeline-parallel-size","1",

    #         # deepdeek-r1
    #         # "--quantization","awq",
    #         # "--dtype","float16",
    #         "--gpu-memory-utilization","0.95",
    #         # "--max-num-seqs","256",
    #         # "--enable-prefix-caching",

    #         # "--tensor-parallel-size","2",

    #         # if OOM
    #         # "--kv-cache-dtype","fp8",
    #         # "--max-model-len","2048",
    #         # "--tensor-parallel-size","1",
    #         # "--max-num-batched-tokens","1",
    #         # "--max-num-seqs","1",
    #         # "--gpu-memory-utilization","0.99",

    #         # If really OOM:
    #         # "--enforce-eager",
    #         # "--enable-chunked-prefill",
    #         "--cpu-offload-gb","50",
    #         # "--load-format","fastsafetensors",

    #         # "--speculative-config",'{"model": "Qwen/Qwen3-0.6B-FP8", "num_speculative_tokens": 5, "prompt_lookup_max": 4, "method": "ngram"}',
    #     ]
    #     environment:
    #         # - VLLM_USE_MODELSCOPE=true
    #         # openai o4-mini
    #         # - OMP_NUM_THREADS=4                     # keep CPU threads in check
    #         # - MKL_NUM_THREADS=4                     # avoid MKL hyperscaling
    #         # - NCCL_DEBUG=INFO                        # diagnose any slowdowns
    #         # - CUDA_LAUNCH_BLOCKING=0                # asynchronous launches
    #         # - TF_CPP_MIN_LOG_LEVEL=2                # suppress verbose logs

    #         # deepseek-r1
    #         # - VLLM_INSTALL_FLASH_ATTN=1
    #         # - NCCL_IGNORE_DISABLED_P2P=1
    #         # - USE_FASTSAFETENSOR=true

    #         - VLLM_DO_NOT_TRACK=1
    #         - VLLM_NO_USAGE_STATS=1
    #         # - VLLM_ATTENTION_BACKEND=FLASHINFER
    #         # - VLLM_ATTENTION_BACKEND=FLASH_ATTN
    #         - HF_HUB_DOWNLOAD_TIMEOUT=300
    #         - HF_HUB_ETAG_TIMEOUT=300
    #         - HF_HUB_ENABLE_HF_TRANSFER=1
    #         # - HF_HUB_ENABLE_HF_TRANSFER=0
    #     ipc: host
    #     # shm_size: '64gb'
    #     # healthcheck:
    #     #     test: [ "CMD", "curl", "-f", "http://0.0.0.0:5000/v1/models" ]
    #     #     interval: 30s
    #     #     timeout: 5s
    #     #     retries: 20
    #     stdin_open: true
    #     tty: true

    infinity:
        ports:
            - 8881:8881
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: 32
                  memory: 2g
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - /home/alex/.cache/huggingface:/app/.cache/huggingface
            - infinity-cache:/app/.cache
        environment:
            - DO_NOT_TRACK=1
            - INFINITY_ANONYMOUS_USAGE_STATS=0
            # - INFINITY_MODEL_ID=jinaai/jina-clip-v2 # slow because no xformers
            # - INFINITY_MODEL_ID=jinaai/jina-clip-v1
            - INFINITY_MODEL_ID=openai/clip-vit-base-patch32;laion/larger_clap_general
            # - INFINITY_MODEL_ID=wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M;jinaai/jina-clip-v1;jinaai/jina-clip-v2
            # - INFINITY_MODEL_ID=nielsr/imagebind-huge # not supported
            # - INFINITY_MODEL_ID=dunzhang/stella_en_400M_v5
            # - INFINITY_REVISION=refs/pr/24
            # - INFINITY_MODEL_ID=KeyurRamoliya/multilingual-e5-large-instruct-GGUF
            # - INFINITY_REVISION=main
            # - INFINITY_DTYPE=bfloat16
            # - INFINITY_BATCH_SIZE=16
            - INFINITY_ENGINE=torch
            - INFINITY_DEVICE=cuda
            # - INFINITY_ENGINE=optimum
            # - INFINITY_DEVICE=tensorrt
            - INFINITY_PORT=8881
            - INFINITY_MODEL_WARMUP=true
            - INFINITY_VECTOR_DISK_CACHE=true
            - INFINITY_COMPILE=fale
            - INFINITY_BETTERTRANSFORMER=true
            - INFINITY_TRUST_REMOTE_CODE=true
            - INFINITY_API_KEY=asdf
        image: michaelf34/infinity:latest-trt-onnx
        container_name: infinity
        restart: unless-stopped
        networks:
            - ollama-docker
        # command: v2 --engine optimum --device cuda --model-id  --port 8881
        # command: v2 --model-id dunzhang/stella_en_400M_v5 --revision "refs/pr/24" --dtype bfloat16 --batch-size 16 --device cuda --engine torch --port 8881 --no-bettertransformer
        command: v2
        stdin_open: true
        tty: true

    netdata:
        ports:
          - 8882:19999
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: 1
                  memory: 300m
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
          - /home/alex/ollama-docker-stack/configs/netdata/netdata/config:/etc/netdata
          # - netdataconfig:/etc/netdata
          - netdatalib:/var/lib/netdata
          - netdatacache:/var/cache/netdata
          - /:/host/root:ro,rslave
          - /etc/passwd:/host/etc/passwd:ro
          - /etc/group:/host/etc/group:ro
          - /etc/localtime:/etc/localtime:ro
          - /proc:/host/proc:ro
          - /sys:/host/sys:ro
          - /etc/os-release:/host/etc/os-release:ro
          - /var/log:/host/var/log:ro
          - /var/run/docker.sock:/var/run/docker.sock:ro
          - /run/dbus:/run/dbus:ro
        image: netdata/netdata:stable
        container_name: netdata
        restart: unless-stopped
        networks:
            - ollama-docker
        cap_add:
          - SYS_PTRACE
          - SYS_ADMIN
        pid: host
        security_opt:
          - apparmor:unconfined
        stdin_open: true
        tty: true

    watchtower:
        deploy:
            resources:
                limits:
                  cpus: '1'
                  memory: 50m
        volumes:
          - /var/run/docker.sock:/var/run/docker.sock
        image: containrrr/watchtower:latest
        container_name: watchtower
        restart: unless-stopped
        networks:
            - ollama-docker
        command: --interval 300
        stdin_open: true
        tty: true

    jupyter-cuda:
        ports:
            - 8888:8888
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: '2'
                  memory: 2G
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
          - jupyter-data:/home/jovyan/work
        environment:
          - GRANT_SUDO=yes
          - JUPYTER_ENABLE_LAB=yes
          - JUPYTER_TOKEN=fa8a78754e9bad79cc5939d359d12ff0b9d41ccb6ee34c68
          # - NB_UID="$(id -u)"
          # - NB_GID="$(id -g)"
        image: cschranz/gpu-jupyter:v1.9_cuda-12.6_ubuntu-24.04
        container_name: jupyter-cuda
        restart: unless-stopped
        networks:
            - ollama-docker
        user: root
        stdin_open: true
        tty: true

    openhands:
        ports:
            - 8887:3000
        extra_hosts:
            - host.docker.internal:host-gateway
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: '4'
                  memory: 4G
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - openhands-state:/.openhands-state
            - workspace:/workspace
            - file_store:/file_store
            - /home/alex/ollama-docker-stack/configs/openhands/config.toml:/app/openhands/config.toml
            - /home/alex/ollama-docker-stack/configs/openhands/config.toml:/app/openhands/core/config.toml
            - /home/alex/ollama-docker-stack/configs/openhands/config.toml:/app/openhands/core/config/config.toml
        environment:
            # - LLM_BASE_URL=http://tabbyapi:5000/v1
            # - LLM_CUSTOM_LLM_PROVIDER=ollama
            # - LLM_EMBEDDING_BASE_URL=http://ollama:11434
            # - LLM_EMBEDDING_MODEL=hf.co/gpustack/jina-embeddings-v2-base-en-GGUF:Q8_0
            # - LLM_MODEL=ollama/hf.co/unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF:Q6_K
            # - LLM_MODEL=openai/bartowski-Qwen2.5-Coder-32B-Instruct-exl2-4.0bpw
            # - LLM_MODEL=openai/hf.co/arcee-ai/Arcee-Blitz-GGUF:IQ4_NL_32768
            # - LLM_MODEL=ollama/hf.co/bartowski/cognitivecomputations_Dolphin3.0-R1-Mistral-24B-GGUF:Q6_K_L
            - SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:latest-nikolaik
            - LOG_ALL_EVENTS=true
            - WORKSPACE_BASE=/workspace
            - LLM_API_KEY=asdf
            #- LLM_BASE_URL=http://ollama:11434/v1
            - LLM_BASE_URL=http://llama-swap:8080/v1/
            - LLM_EMBEDDING_BASE_URL=http://infinity:8881
            - LLM_EMBEDDING_MODEL=dunzhang/stella_en_400M_v5
            #- LLM_MODEL=openai/hf.co/arcee-ai/Arcee-Blitz-GGUF:Q6_K_L
            - LLM_MODEL=openai/devstral-q4-q4-q4-128k
            - LLM_CACHING_PROMPT=true
            - AGENT_MEMORY_ENABLED=true
            - REASONING_EFFORT=high
            - MAX_ITERATIONS=1000
            - LLM_TEMPERATURE=0.15
            - AGENT_MEMORY_MAX_THREADS=32
            - AGENT_ENABLE_AUTO_LINT=true
            #- LLM_OLLAMA_BASE_URL=http://ollama:11434
        # depends_on:
        #     # - ollama
        #     - infinity
        image: docker.all-hands.dev/all-hands-ai/openhands:latest
        container_name: openhands-app
        restart: unless-stopped
        networks:
            - ollama-docker
        stdin_open: true
        tty: true
    tika:
        ports:
          - 9998:9998
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: '1'
                  memory: 300m
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        image: apache/tika:latest-full
        container_name: tika
        restart: unless-stopped
        networks:
            - ollama-docker
        stdin_open: true
        tty: true
    llama-swap:
        ports:
            - 9292:8080
        runtime: nvidia
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        image: ghcr.io/mostlygeek/llama-swap:cuda
        command: /app/llama-swap -config /app/config.yaml -watch-config
        volumes:
            #  keep python/hf-transfer installed
            - llama-swap-usr:/usr
            - /home/alex/.cache/models:/models
            - /home/alex/.cache/models:/root/.cache/llama.cpp
            - /home/alex/ollama-docker-stack/configs/llama-swap/config.yaml:/app/config.yaml
        environment:
            - HF_HUB_ENABLE_HF_TRANSFER=1
        container_name: llama-swap
        restart: unless-stopped
        networks:
            - ollama-docker
        stdin_open: true
        tty: true
    # ollama:
    #     ports:
    #         - 8002:11434
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             limits:
    #               cpus: '32'
    #               memory: 64G
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - /home/alex/ollama-docker-stack/configs/ollama-custom-models:/custom-models
    #         - /home/alex/.cache/ollama:/root/.ollama
    #         - /home/alex/.cache/ollama:/usr/share/ollama/.ollama
    #         - /home/alex/.cache/huggingface:/root/.cache/huggingface
    #         - dot-cache:/root/.cache
    #     environment:
    #         - OLLAMA_CONTEXT_LENGTH=32768
    #         - OLLAMA_ORIGINS=moz-extension://*
    #         - OLLAMA_FLASH_ATTENTION=1
    #         # q4 has bug: https://github.com/ollama/ollama/issues/7938
    #         - OLLAMA_KV_CACHE_TYPE=q4_0
    #         # - OLLAMA_KV_CACHE_TYPE=q6_0
    #         # - OLLAMA_KV_CACHE_TYPE=q8_0
    #         - OLLAMA_MAX_VRAM=25769803776
    #         - OLLAMA_KEEP_ALIVE=300
    #         - OLLAMA_LOAD_TIMEOUT=300
    #         - GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
    #         - OLLAMA_NUM_PARALLEL=1
    #         - OLLAMA_MAX_QUEUE=128
    #         # - OLLAMA_MAX_VRAM=25769803776
    #         # - "OLLAMA_KV_CACHE_TYPE=q8_0
    #         # - "OLLAMA_MAX_VRAM=23192823398
    #         # 24 GB in bytes 24*(1024^3)
    #         # - "OLLAMA_MAX_LOADED_MODELS=3"
    #         # - "OLLAMA_GPU_OVERHEAD=536870912
    #         # 12 GB in bytes
    #         # - "OLLAMA_MAX_VRAM=12884901888
    #     container_name: ollama
    #     restart: unless-stopped
    #     image: ollama/ollama:latest
    #     networks:
    #         - ollama-docker
    #     stdin_open: true
    #     tty: true
    speaches:
        ports:
            - 8004:8000
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: '2'
                  memory: 2G
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - /home/alex/.cache/huggingface:/home/ubuntu/.cache/huggingface
            - dot-cache:/home/ubuntu/.cache
        environment:
            # - "WHISPER__MODEL=deepdml/faster-whisper-large-v3-turbo-ct2
            # - "WHISPER__MODEL=ctranslate2-4you/whisper-tiny.en-ct2-int8
            # - WHISPER__MODEL=Systran/faster-whisper-small
            - WHISPER__MODEL=deepdml/faster-whisper-large-v3-turbo-ct2
            - ENABLE_UI=False
            - USE_BATCHED_MODE=True
            - LOG_LEVEL=INFO
            - CHAT_COMPLETION_BASE_URL=http://ollama:11434
        container_name: speaches
        restart: unless-stopped
        image: ghcr.io/speaches-ai/speaches:latest-cuda
        networks:
          - ollama-docker
        stdin_open: true
        tty: true
    searxng:
        ports:
            # - "127.0.0.1:8080:8080"
            - 8080:8080
        deploy:
            resources:
                limits:
                  cpus: '2'
                  memory: 1g
        volumes:
            - /home/alex/ollama-docker-stack/configs/searxng:/etc/searxng:rw
        environment:
            - SEARXNG_PORT=8080
            - SEARXNG_BIND_ADDRESS=0.0.0.0
            - SEARXNG_HOSTNAME=searxng:8080
            - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
            - UWSGI_WORKERS=2
            - UWSGI_THREADS=10
        container_name: searxng
        restart: unless-stopped
        image: docker.io/searxng/searxng:latest
        networks:
            - ollama-docker
        stdin_open: true
        tty: true
        # cap_drop:
        #     - ALL
        cap_add:
            - CHOWN
            - SETGID
            - SETUID
            - DAC_OVERRIDE
        logging:
            driver: "json-file"
            options:
                max-size: "1m"
                max-file: "1"

    open-webui:
        ports:
            - 8005:8080
        runtime: nvidia
        deploy:
            resources:
                limits:
                  cpus: '4'
                  memory: 2G
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - /home/alex/.cache/huggingface:/root/.cache/huggingface
            - dot-cache:/root/.cache
            - webui-code:/app/backend/data
        environment:
            - ENABLE_IMAGE_GENERATION=true
            - COMFYUI_BASE_URL=http://localhost:8005
            - ENABLE_GOOGLE_DRIVE_INTEGRATION=True
            - ENABLE_RAG_WEB_SEARCH=true
            - RAG_WEB_SEARCH_ENGINE=searxng
            - RAG_WEB_SEARCH_RESULT_COUNT=3
            - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
            - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
        # depends_on:
        #     # - comfyui-cuda
        #     # - ollama
        #     - speaches
        #     # - pipelines
        #     - searxng
        #     - tika
        #     # - infinity
        #     - jupyter-cuda
        healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
            interval: 30s
            timeout: 10s
            retries: 3
        container_name: open-webui
        restart: unless-stopped
        image: ghcr.io/open-webui/open-webui:cuda
        networks:
            - ollama-docker
        stdin_open: true
        tty: true

networks:
    ollama-docker:

volumes:
    llama-swap-usr:
    ollama-volume:
    comfy-ui-root:
    dot-cache:
    webui-code:
    pipelines:
    pipeline-usr:
    caddy-data:
    caddy-config:
    valkey-data2:
    openhands-state:
    workspace:
    file_store:
    jupyter-data:
    tabbyAPI-models:
    tabbyDist:
    infinity-cache:
    netdataconfig:
    netdatalib:
    netdatacache:
    vllm-cache:
    jupyter-jovyan:
    jupyter-opt:
    # whisperlive-app:
    # whisperlive-usr:
    # whisper-cache:
    # speaches-folder:

    # gpt-researcher:
    #     ports:
    #         - 8000:8000
    #     volumes:
    #         - /home/alex/ollama-docker-stack/volumes/gptr/my-docs:/usr/src/app/my-docs:rw
    #         - /home/alex/ollama-docker-stack/volumes/gptr/outputs:/usr/src/app/outputs:rw
    #         - /home/alex/ollama-docker-stack/volumes/gptr/logs:/usr/src/app/logs:rw
    #     environment:
    #         RETRIEVER: "searx"
    #         SEARX_URL: "http://searxng:8080/"
    #         EMBEDDING: "custom:dunzhang/stella_en_400M_v5"
    #         FAST_LLM: "ollama:hf.co/bartowski/SmolLM2-1.7B-Instruct-GGUF:IQ4_XS"
    #         SMART_LLM: "ollama:hf.co/bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF:IQ4_XS"
    #         STRATEGIC_LLM: "ollama:hf.co/bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF:IQ4_XS:thinking"
    #         CURATE_SOURCES: "true"
    #         TEMPERATURE: "0.01"
    #         MAX_ITERATIONS: "3"
    #         MAX_SUBTOPICS: "3"
    #         # SCRAPER: "browser"
    #         SCRAPER: "nodriver"
    #         MAX_SCRAPER_WORKERS: "15"
    #         LOGGING_LEVEL: "INFO"

    #         OPENAI_API_KEY: "asdf"
    #         OLLAMA_BASE_URL: "http://ollama:11434"
    #         OPENAI_BASE_URL: "http://infinity:8881/v1"
    #     image: gptresearcher/gpt-researcher:latest
    #     container_name: gpt-researcher
    #     restart: unless-stopped
    #     networks:
    #         - ollama-docker
    #     user: root
    #     stdin_open: true
    #     tty: true

    # gptr-nextjs:
    #     ports:
    #         - 3000:3000
    #     volumes:
    #         - /app/node_modules
    #         - ./frontend/nextjs:/app
    #         - ./frontend/nextjs/.next:/app/.next
    #         - ./outputs:/app/outputs
    #     environment:
    #         CHOKIDAR_USEPOLLING: "true"
    #         LOGGING_LEVEL: INFO
    #         # NEXT_PUBLIC_GA_MEASUREMENT_ID: ${NEXT_PUBLIC_GA_MEASUREMENT_ID}
    #         # NEXT_PUBLIC_GPTR_API_URL: ${NEXT_PUBLIC_GPTR_API_URL}
    #     image: gptresearcher/gptr-nextjs:latest
    #     container_name: gptr-nextjs
    #     restart: unless-stopped
    #     networks:
    #         - ollama-docker
    #     stdin_open: true
    #     tty: true
    # aphrodite-openai:
    #     ports:
    #         - 8886:2242
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         # - ~/.cache/huggingface:/root/.cache/huggingface
    #         - dot-cache:/root/.cache
    #     ipc: host
    #     # image: alpindale/aphrodite-openai:latest

    #     # 0.5.3 is the latest with exl2 support, it was dropped for some reason...
    #     image: alpindale/aphrodite-openai:latest
    #     container_name: aphrodite-openai
    #     # restart: unless-stopped
    #     # command: --model NousResearch/Meta-Llama-3.1-8B-Instruct --tensor-parallel-size 8 --api-keys "sk-empty"
    #     # command: --model bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF:IQ4_XS --api-keys "asdf" --trust-remote-code -tp 2
    #     #command: --host "0.0.0.0" --trust-remote-code --uvloop --model-loader-extra-config /home/alex/ollama-docker-stack/configs/aphrodite/deepcogito_cogito-v1-preview-qwen-32B-GGUF-config.json --model bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF
    #     command: [
    #       "--host", "0.0.0.0",
    #       "--trust-remote-code",
    #       "--max-num-seqs", "1",
    #       # "--model", "bartowski/deepcogito_cogito-v1-preview-qwen-32B-GGUF",
    #       # "--model", "deepcogito/cogito-v1-preview-llama-3B",
    #       "--model", "deepcogito/cogito-v1-preview-qwen-32B",
    #       # "--quantization", "IQ4_XS",
    #       # "--config", "/home/alex/ollama-docker-stack/configs/aphrodite/deepcogito_cogito-v1-preview-qwen-32B-GGUF/config.json",
    #       # "--config-format", "hf"
    #       "--cpu-offload-gb", "60",
    #       "--swap-space", "1",
    #       "--gpu-memory-utilization", "0.99",

    #       "--max-model-len", "2048",
    #       "--max-seq-len-to-capture", "2048",
    #       "--max-num-batched-tokens", "2048",
    #       # "--max-context-len-to-capture", "2048",
    #       # "--kv-cache-dtype", "fp8",
    #       "--single-user-mode",
    #       "--device", "cuda",

    #       # "--speculative-model", "deepcogito/cogito-v1-preview-llama-3B",
    #       # "--num-speculative-tokens", "2",
    #       # "--speculative-draft-tensor-parallel-size", "1",
    #       # "--num-lookahead-slots", "2",
    #       # "--speculative-max-model-len", "16000",
    #       # "--enable-chunked-prefill",

    #       # "--enable-prefix-caching",
    #     ]
    #     environment:
    #         - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    #       # - APHRODITE_FORCE_SINGLE_USER_PREFIX_CACHE=1
    #     stdin_open: true
    #     tty: true

    # tabbyAPI:
    #     ports:
    #       - 8886:5000
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             limits:
    #               cpus: '32'
    #               memory: 64G
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #       - tabbyAPI-models:/app/models
    #       - /home/alex/ollama-docker-stack/configs/tabbyapi/config.yml:/app/config.yml
    #       - tabbyDist:/usr/local/lib/python3.10/dist-packages
    #     image: ghcr.io/theroyallab/tabbyapi:latest
    #     container_name: tabby-api
    #     restart: unless-stopped
    #     networks:
    #         - ollama-docker
    #     healthcheck:
    #       test: ["CMD", "curl", "-f", "http://127.0.0.1:5000/health"]
    #       interval: 30s
    #       timeout: 10s
    #       retries: 3
    #     # command: /usr/bin/python3 -m pip install -U xformers && /usr/bin/python3 /app/main.py
    #     # command: /bin/sh -c '/usr/bin/python3 -m pip install -U xformers && /usr/bin/python3 /app/main.py'
    #     # command: -m pip install -U xformers
    #     environment:
    #       - NAME=TabbyAPI
    #       - NVIDIA_VISIBLE_DEVICES=all
    #       - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    #     user: root
    #     stdin_open: true
    #     tty: true

    # pipelines:
    #     ports:
    #         - 8009:9099
    #     deploy:
    #         restart_policy:
    #             condition: "none"
    #     # runtime: nvidia
    #     # deploy:
    #     #     resources:
    #     #         reservations:
    #     #             devices:
    #     #                 - driver: nvidia
    #     #                   count: all
    #     #                   capabilities:
    #     #                       - gpu
    #     volumes:
    #         - pipelines:/pipelines
    #         - pipeline-usr:/usr
    #     # environment:
    #     #     - PIPELINES_DIR=/app/pipelines
    #     #     - PIPELINES_URLS=https://github.com/open-webui/pipelines/blob/main/examples/pipelines/integrations/python_code_pipeline.py"
    #     container_name: pipelines
    #     restart: "no"
    #     image: ghcr.io/open-webui/pipelines:latest-cuda
    #     networks:
    #         - ollama-docker

    # comfyui-cuda:
    #     ports:
    #         - 8001:8188
    #     runtime: nvidia
    #     deploy:
    #         restart_policy:
    #             condition: "none"
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - comfy-ui-root:/root
    #         # - dot-cache:/root/.cache
    #         # - "C:/Users/xfant/.cache:/root/.cache"
    #         # - "C:/Program Files (x86)/ComfyUI_windows_portable/ComfyUI:/root/ComfyUI"
    #         # - "C:/Users/xfant/ollama-ui-speeches-stack/comfy-ui-root:/root"
    #         # - comfy-ui-app:/root/ComfyUI
    #     environment:
    #         - "CLI_ARGS=--fast --fp8_e4m3fn-unet --fp8_e4m3fn-text-enc --bf16-vae --use-pytorch-cross-attention --lowvram"
    #         - "HF_TOKEN=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"
    #     container_name: comfyui-cuda
    #     restart: "no"
    #     image: yanwk/comfyui-boot:cu124-slim
    #     networks:
    #         - ollama-docker

    # libretranslate:
    #     ports:
    #         - 5000:5000
    #     # runtime: nvidia
    #     # deploy:
    #     #     resources:
    #     #         reservations:
    #     #             devices:
    #     #                 - driver: nvidia
    #     #                   count: all
    #     #                   capabilities:
    #     #                       - gpu
    #     container_name: libretranslate
    #     restart: unless-stopped
    #     image: libretranslate/libretranslate:latest
    #     networks:
    #         - ollama-docker

    # caddy:
    #     container_name: caddy
    #     image: docker.io/library/caddy:2-alpine
    #     network_mode: host
    #     restart: unless-stopped
    #     volumes:
    #         - ./Caddyfile:/etc/caddy/Caddyfile:ro
    #         - caddy-data:/data:rw
    #         - caddy-config:/config:rw
    #     environment:
    #         - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-http://localhost:80}
    #         - SEARXNG_TLS=${LETSENCRYPT_EMAIL:-internal}
    #     cap_drop:
    #         - ALL
    #     cap_add:
    #         - NET_BIND_SERVICE
    #     logging:
    #         driver: "json-file"
    #         options:
    #             max-size: "1m"
    #             max-file: "1"

    # redis:
    #     volumes:
    #         - valkey-data2:/data
    #     container_name: redis
    #     restart: unless-stopped
    #     image: docker.io/valkey/valkey:8-alpine
    #     command: valkey-server --save 30 1 --loglevel warning
    #     networks:
    #         - searxng
    #     # cap_drop:
    #     #     - ALL
    #     cap_add:
    #         - SETGID
    #         - SETUID
    #         - DAC_OVERRIDE
    #     logging:
    #         driver: "json-file"
    #         options:
    #             max-size: "1m"
    #             max-file: "1"

    # # tts, fast
    # kokoro-fastapi:
    #     ports:
    #         # API
    #         - 8006:8880
    #         # UI
    #         - 8007:7860
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     # volumes:
    #     #     - "C:/Users/xfant/ollama-ui-speeches-stack/kokoro-fast-api-app:/app"
    #     environment:
    #         - DISABLE_LOCAL_SAVING=true
    #     container_name: kokoro-fastapi
    #     restart: unless-stopped
    #     image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
    #     networks:
    #         - ollama-docker

    # openai-whisper-asr-webservice:
    #     ports:
    #         - 8010:9000
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - whisper-cache:/root/.cache/whisper
    #     environment:
    #         - ASR_ENGINE=faster_whisper
    #         - ASR_MODEL=tiny
    #         - MODEL_IDLE_TIMEOUT=300
    #     container_name: openai-whisper-asr-webservice
    #     image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    #     networks:
    #         - ollama-docker

    # alltalk-tts:
    #     ports:
    #         - "8011:7851"
    #         - "8012:7852"
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - "C:/Users/xfant/ollama-ui-speeches-stack/outputs:/app/outputs/"
    #         - "C:/Users/xfant/ollama-ui-speeches-stack/models:/app/models/"
    #         - "C:/Users/xfant/ollama-ui-speeches-stack/voices:/app/voices/"
    #         - "C:/Users/xfant/ollama-ui-speeches-stack/finetune/put-voice-samples-in-here:/app/finetune/put-voice-samples-in-here"
    #         - "C:/Users/xfant/ollama-ui-speeches-stack/dockerconfig.json:/app/confignew.json"
    #     restart: unless-stopped
    #     image: lcomunicamos/alltalk:cuda
    #     networks:
    #         - ollama-docker

    # whisperlive-gpu:
    #     ports:
    #         - 8008:9090
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - whisperlive-app:/app
    #         - whisperlive-usr:/usr
    #     command: bash -c '/usr/bin/python3 run_server.py --port 9090 --backend faster_whisper'
    #     container_name: whisperlive-gpu
    #     restart: unless-stopped
    #     image: ghcr.io/collabora/whisperlive-gpu:latest
    #     networks:
    #         - ollama-docker

    # whisperlive-tensorrt:
    #     ports:
    #         - 8008:9090
    #     runtime: nvidia
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       count: all
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - whisperlive-app:/app
    #         - whisperlive-usr:/usr
    #     command: bash -c '/usr/bin/python3 run_server.py --port 9090 --backend tensorrt --trt_model_path "/app/TensorRT-LLM-examples/whisper/whisper_small_en_float16" --trt_model_path "/app/TensorRT-LLM-examples/whisper/whisper_small_en_int8" --trt_model_path "/app/TensorRT-LLM-examples/whisper/whisper_small_en_int4"'
    #     container_name: whisperlive-tensorrt
    #     restart: unless-stopped
    #     image: ghcr.io/collabora/whisperlive-tensorrt
    #     networks:
    #         - ollama-docker
